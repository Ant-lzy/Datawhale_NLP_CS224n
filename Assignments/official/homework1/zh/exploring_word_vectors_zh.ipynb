{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datawhale_NLP_CS224n Assignment 1: Exploring Word Vectors (25 Points)\n",
    "\n",
    "\n",
    "Welcome to Datawhale_NLP_CS224n! \n",
    "\n",
    "Before you start, make sure you read the README.txt in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_list:1000,train_labels1000\n",
      "test_list:1000,test_labels1000\n"
     ]
    }
   ],
   "source": [
    "demo_flag = True   # 是否处于 测试  True : 测试   False : 非测试\n",
    "data_path = 'resource/'\n",
    "\n",
    "train_file_path = data_path+\"t1_cut_words_cnews.train.txt\"\n",
    "train_list, train_labels = tools.read_file(train_file_path,demo_flag = demo_flag)\n",
    "print(\"train_list:{0},train_labels{1}\".format(len(train_list),len(train_labels)))\n",
    "# print(\"train_list:{0},train_labels{1}\".format(train_list[0:1], train_labels[0:1]))\n",
    "\n",
    "test_file_path = data_path+\"t1_cut_words_cnews.test.txt\"\n",
    "test_list, test_labels = tools.read_file(test_file_path, demo_flag=demo_flag)\n",
    "print(\"test_list:{0},test_labels{1}\".format(len(test_list), len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征词转化为 One-hot 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 用选取的特征词构建0-1矩阵\n",
    "def text_features(train_data_list, test_data_list, feature_words):\n",
    "    '''\n",
    "    # 用选取的特征词构建0-1矩阵\n",
    "    # 对训练数据集train_data_list中每篇切完词之后的文档构建特征向量（\n",
    "    # 由上述1000个特征词组成），若出现则取值为1，否则为0。\n",
    "    # 于是文章构建出了[90,1000]维度的0-1矩阵。\n",
    "    :param train_data_list:     list        训练集列表\n",
    "    :param test_data_list:      list        测试集列表\n",
    "    :param feature_words:       list        特征词列表\n",
    "    :return:\n",
    "    '''\n",
    "    def text_features_process(text, feature_words):\n",
    "        # text = train_data_list[0]\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "\n",
    "    # 0,1的矩阵（1000列-维度）\n",
    "    train_feature_list = [text_features_process(text.split(\" \"), feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features_process(text.split(\" \"), feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "all_words_list = tools.count_words(train_list)\n",
    "# print(f\"all_words_list:{all_words_list}\")\n",
    "\n",
    "# 选取1000个特征\n",
    "deleteN = 5000\n",
    "feature_words = tools.words_dict(all_words_list, deleteN)\n",
    "# print(\"feature_words：{0}\".format(feature_words))\n",
    "\n",
    "# 计算特征向量\n",
    "train_feature_list, test_feature_list = text_features(train_list, test_list, feature_words)\n",
    "\n",
    "feature_dict_filename = data_path+\"t2_feature_words_cnews.txt\"\n",
    "tools.write_feature_dict_file(feature_dict_filename, feature_words)\n",
    "\n",
    "train_feature_filename = data_path+\"t2_text_feature_cnews.train.txt\"\n",
    "tools.write_file(train_feature_filename, train_feature_list, train_labels, is_num=True)\n",
    "\n",
    "test_feature_filename = data_path+\"t2_text_feature_cnews.test.txt\"\n",
    "tools.write_file(test_feature_filename, test_feature_list, test_labels, is_num=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征词转化为 tdidf 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 特征词转化为 tdidf 矩阵,并提取 max_features 个特征\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def text_features_by_tfidf(train_data,test_data,max_features=6000):\n",
    "    '''\n",
    "    特征词转化为 tdidf 矩阵,并提取 max_features 个特征\n",
    "    :param train_data:      list     训练数据及\n",
    "    :param test_data:       list      测试数据集\n",
    "    :param max_features:    int   选取最大特征数目\n",
    "    :return:\n",
    "        train_x ：   list\n",
    "        test_x  ：   list\n",
    "    '''\n",
    "    count = CountVectorizer()\n",
    "    count.fit_transform(train_data)\n",
    "    tfidf_transformer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_transformer.fit(train_data)\n",
    "\n",
    "    # 将train_data，test_data转换成tfidf矩阵\n",
    "    train_x = tfidf_transformer.transform(train_data).toarray()\n",
    "    test_x = tfidf_transformer.transform(test_data).toarray()\n",
    "    words = count.get_feature_names()\n",
    "    return train_x,test_x,words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取1000个特征\n",
    "deleteN = 5000\n",
    "\n",
    "# 计算特征向量\n",
    "train_feature_list, test_feature_list, feature_words = text_features_by_tfidf(train_data = train_list,test_data = test_list,max_features = deleteN)\n",
    "\n",
    "#print(\"feature_words:{0}\".format(feature_words))\n",
    "\n",
    "feature_dict_filename = data_path+\"t2_feature_words_tfidf_cnews.txt\"\n",
    "tools.write_feature_dict_file(feature_dict_filename, feature_words)\n",
    "\n",
    "train_feature_filename = data_path+\"t2_text_feature_tfidf_cnews.train.txt\"\n",
    "train_feature_list = list(map(list, train_feature_list))\n",
    "tools.write_file(train_feature_filename, train_feature_list, train_labels, is_num=True)\n",
    "\n",
    "test_feature_filename = data_path+\"t2_text_feature_tfidf_cnews.test.txt\"\n",
    "test_feature_list = list(map(list, test_feature_list))\n",
    "tools.write_file(test_feature_filename, test_feature_list, test_labels, is_num=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用 word2vec 进行 词向量训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-26 12:21:15,450:INFO: collecting all words and their counts\n",
      "2020-04-26 12:21:15,455:INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-04-26 12:21:17,818:INFO: collected 150537 word types from a corpus of 5451783 raw words and 546 sentences\n",
      "2020-04-26 12:21:17,819:INFO: Loading a fresh vocabulary\n",
      "2020-04-26 12:21:18,043:INFO: effective_min_count=5 retains 47163 unique words (31% of original 150537, drops 103374)\n",
      "2020-04-26 12:21:18,044:INFO: effective_min_count=5 leaves 5280396 word corpus (96% of original 5451783, drops 171387)\n",
      "2020-04-26 12:21:18,210:INFO: deleting the raw counts dictionary of 150537 items\n",
      "2020-04-26 12:21:18,217:INFO: sample=0.001 downsamples 26 most-common words\n",
      "2020-04-26 12:21:18,218:INFO: downsampling leaves estimated 4303497 word corpus (81.5% of prior 5280396)\n",
      "2020-04-26 12:21:18,385:INFO: estimated required memory for 47163 words and 200 dimensions: 99042300 bytes\n",
      "2020-04-26 12:21:18,387:INFO: resetting layer weights\n",
      "2020-04-26 12:21:30,658:INFO: training model with 3 workers on 47163 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-26 12:21:31,663:INFO: EPOCH 1 - PROGRESS: at 16.67% examples, 715221 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:32,666:INFO: EPOCH 1 - PROGRESS: at 32.42% examples, 690681 words/s, in_qsize 4, out_qsize 1\n",
      "2020-04-26 12:21:33,686:INFO: EPOCH 1 - PROGRESS: at 50.00% examples, 708472 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:34,704:INFO: EPOCH 1 - PROGRESS: at 67.95% examples, 719739 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:35,709:INFO: EPOCH 1 - PROGRESS: at 86.08% examples, 732560 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:36,484:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-26 12:21:36,496:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-26 12:21:36,502:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-26 12:21:36,503:INFO: EPOCH - 1 : training on 5451783 raw words (4303678 effective words) took 5.8s, 736753 effective words/s\n",
      "2020-04-26 12:21:37,507:INFO: EPOCH 2 - PROGRESS: at 16.67% examples, 715860 words/s, in_qsize 4, out_qsize 1\n",
      "2020-04-26 12:21:38,507:INFO: EPOCH 2 - PROGRESS: at 34.62% examples, 738726 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:39,509:INFO: EPOCH 2 - PROGRESS: at 50.92% examples, 726682 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:40,514:INFO: EPOCH 2 - PROGRESS: at 67.95% examples, 725880 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:41,516:INFO: EPOCH 2 - PROGRESS: at 84.80% examples, 726779 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:42,343:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-26 12:21:42,346:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-26 12:21:42,361:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-26 12:21:42,363:INFO: EPOCH - 2 : training on 5451783 raw words (4303606 effective words) took 5.9s, 734783 effective words/s\n",
      "2020-04-26 12:21:43,381:INFO: EPOCH 3 - PROGRESS: at 18.32% examples, 775022 words/s, in_qsize 4, out_qsize 1\n",
      "2020-04-26 12:21:44,391:INFO: EPOCH 3 - PROGRESS: at 35.35% examples, 745272 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:45,393:INFO: EPOCH 3 - PROGRESS: at 51.65% examples, 731126 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:46,400:INFO: EPOCH 3 - PROGRESS: at 68.13% examples, 723159 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:47,421:INFO: EPOCH 3 - PROGRESS: at 86.45% examples, 734765 words/s, in_qsize 4, out_qsize 1\n",
      "2020-04-26 12:21:48,126:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-26 12:21:48,142:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-26 12:21:48,146:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-26 12:21:48,148:INFO: EPOCH - 3 : training on 5451783 raw words (4303273 effective words) took 5.8s, 744307 effective words/s\n",
      "2020-04-26 12:21:49,162:INFO: EPOCH 4 - PROGRESS: at 18.50% examples, 785048 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:50,170:INFO: EPOCH 4 - PROGRESS: at 37.18% examples, 786325 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:51,176:INFO: EPOCH 4 - PROGRESS: at 52.93% examples, 749550 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-26 12:21:52,185:INFO: EPOCH 4 - PROGRESS: at 70.33% examples, 746555 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-26 12:21:53,189:INFO: EPOCH 4 - PROGRESS: at 88.28% examples, 753246 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:53,785:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-26 12:21:53,787:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-26 12:21:53,801:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-26 12:21:53,802:INFO: EPOCH - 4 : training on 5451783 raw words (4303252 effective words) took 5.7s, 761511 effective words/s\n",
      "2020-04-26 12:21:54,807:INFO: EPOCH 5 - PROGRESS: at 18.32% examples, 784315 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:55,807:INFO: EPOCH 5 - PROGRESS: at 36.26% examples, 772774 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:56,815:INFO: EPOCH 5 - PROGRESS: at 54.40% examples, 774311 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-26 12:21:57,816:INFO: EPOCH 5 - PROGRESS: at 72.34% examples, 772614 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:58,823:INFO: EPOCH 5 - PROGRESS: at 90.66% examples, 777193 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-26 12:21:59,285:INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-26 12:21:59,290:INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-26 12:21:59,292:INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-26 12:21:59,293:INFO: EPOCH - 5 : training on 5451783 raw words (4303132 effective words) took 5.5s, 783958 effective words/s\n",
      "2020-04-26 12:21:59,294:INFO: training on a 27258915 raw words (21516941 effective words) took 28.6s, 751404 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=47163, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s:%(levelname)s: %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(train_file_path)  # 加载语料\n",
    "model = word2vec.Word2Vec(sentences, size=200)  # 训练skip-gram模型，默认window=5\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算两个词的相似度/相关程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【姚明】和【易建联】的相似度为： 0.5219601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y1 = model.similarity(u\"姚明\", u\"易建联\")\n",
    "except KeyError:\n",
    "    y1 = 0\n",
    "print(u\"【姚明】和【易建联】的相似度为：\", y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算某个词的相关词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和【易建联】最相关的词有：\n",
      "\n",
      "布 0.784031867980957\n",
      "米勒 0.7732383012771606\n",
      "毕比 0.7716540098190308\n",
      "韦德 0.7698782086372375\n",
      "洛瑞 0.7667900919914246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y2 = model.most_similar(u\"易建联\", topn=5)  # 20个最相关的\n",
    "print(u\"和【易建联】最相关的词有：\\n\")\n",
    "for item in y2:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "易建联-中国，安东尼-\n",
      "社会 0.44787871837615967\n",
      "海外 0.44349920749664307\n",
      "美国 0.4409010708332062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(u\"易建联-中国，安东尼-\")\n",
    "y3 = model.most_similar([u'易建联', u'中国'], [u'安东尼'], topn=3)\n",
    "for item in y3:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找不合群的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-04-26 12:24:54,520:WARNING: vectors for words {'优秀的'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不合群的词： 易建联\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\progrom\\python\\python\\python3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "y4 = model.doesnt_match(u\"易建联 是 优秀的 运动员\".split())\n",
    "print(u\"不合群的词：\", y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-26 12:25:38,881:INFO: saving Word2Vec object under resource/cnews_vord2vec.model, separately None\n",
      "2020-04-26 12:25:38,882:INFO: not storing attribute vectors_norm\n",
      "2020-04-26 12:25:38,884:INFO: not storing attribute cum_table\n",
      "2020-04-26 12:25:41,174:INFO: saved resource/cnews_vord2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(data_path+\"cnews_vord2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用 one-hot 、TF-idf、word2vec 构建句向量，然后 采用 朴素贝叶斯、条件随机场做分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
